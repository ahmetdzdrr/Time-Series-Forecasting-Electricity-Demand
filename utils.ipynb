{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Libraries**\n",
    "\n",
    "In this project, various libraries have been utilized for different data processing, modeling, and evaluation procedures. Here's a brief overview of these libraries:\n",
    "\n",
    "> ### Data Processing\n",
    "\n",
    "- **Pandas**: An effective library used for data analysis and manipulation. Widely employed to efficiently handle and clean datasets.\n",
    "- **NumPy**: A high-performance library for data manipulation and computations. Especially suitable for matrix operations.\n",
    "\n",
    "> ### Visualization\n",
    "\n",
    "- **Matplotlib**: A fundamental library for data visualization. Used to create various visual presentations such as graphs, histograms, and plots.\n",
    "- **Seaborn**: A visualization library built on Matplotlib, enhancing data visualization to be more aesthetic and straightforward.\n",
    "- **Plotly Express and Plotly Graph Objects**: Libraries used to create interactive and customizable graphs, offering various interactive visualization options.\n",
    "\n",
    "> ### Data Preprocessing and Modeling\n",
    "\n",
    "- **Scikit-learn**: A comprehensive library used for creating, scaling, evaluating, and selecting machine learning models.\n",
    "- **Keras Tuner and Optuna**: Libraries utilized for hyperparameter tuning, enabling the automatic adjustment of different model architectures and parameters.\n",
    "- **LightGBM, XGBoost, CatBoost**: Efficient and fast machine learning libraries containing gradient boosting algorithms. They offer high performance, particularly on large datasets.\n",
    "- **TensorFlow**: A powerful library used for building deep learning and artificial neural network models.\n",
    "\n",
    "These libraries were chosen to perform diverse analyses, experiment with different modeling techniques, and leverage various functionalities for data processing, visualization, modeling, and evaluation within the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Import required modules from scikit-learn\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest, HistGradientBoostingRegressor\n",
    "\n",
    "# Import modules for hyperparameter tuning\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "import optuna\n",
    "\n",
    "# Import machine learning frameworks\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as catb\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Describe Method**\n",
    "\n",
    "This function aims to generate a summary statistics table for a given dataset and display it with color-coded styling to highlight variations. Here's a breakdown of its components:\n",
    "\n",
    "> ### Input Parameters\n",
    "\n",
    "- **dataframe_name**: Represents the name or identifier of the dataset being analyzed.\n",
    "- **dataframe**: Refers to the DataFrame object containing the dataset.\n",
    "Functionality:\n",
    "\n",
    "- **Compute Statistics**: Utilizes the describe() method to calculate descriptive statistics (including count, mean, standard deviation, minimum, various percentiles, and maximum) for numerical columns within the dataset. Percentiles ranging from 0.01 to 0.99 are specified.\n",
    "- **Style Presentation**: Applies a background gradient to the statistics table, with color-coding based on the values, using a light green palette.\n",
    "- **Display**: Presents the styled statistics table using the display() function, allowing a more visual and comprehensive understanding of the dataset's numeric attributes.\n",
    "- **Final Output**: The function returns the original DataFrame, allowing further analysis or processing.\n",
    "\n",
    "> ### Output:\n",
    "\n",
    "The descriptive statistics table, styled with a color gradient, showcasing the distribution and summary of numerical features within the dataset.\n",
    "\n",
    "> ### Purpose:\n",
    "\n",
    "This function aids in quickly examining and understanding the central tendencies, dispersion, and distribution of numerical columns in a given dataset, enabling effective initial exploratory data analysis (EDA).\n",
    "\n",
    "This function is valuable for providing an organized and visually appealing summary of the dataset's numeric attributes, facilitating an initial understanding of the data distribution and statistical characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_dataset(dataframe_name, dataframe):\n",
    "    # Print the name of the checked dataframe\n",
    "    print(f\"Checked {dataframe_name}\")\n",
    "\n",
    "    # Generate descriptive statistics with specified percentiles for the dataframe\n",
    "    describe = dataframe.describe(percentiles=[0.01, 0.05, 0.25, 0.5, 0.75, 0.95, 0.99]).T\n",
    "\n",
    "    # Create a color gradient style for the descriptive statistics\n",
    "    cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "    styled_describe = describe.style.background_gradient(cmap=cm)\n",
    "    \n",
    "    # Display the styled descriptive statistics\n",
    "    display(styled_describe)\n",
    "    print(\"\\n\\n\")  # Add an empty line for better separation\n",
    "    \n",
    "    return dataframe  # Return the original dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Log Method**\n",
    "\n",
    "This LogOperation class encapsulates a processing operation applied to a DataFrame based on a configuration parameter (self.cfg.log_operation). Here's an explanation of its functionality:\n",
    "\n",
    "> ### Attributes\n",
    "\n",
    "- **cfg**: Represents a configuration object that contains parameters for the operation.\n",
    "Methods:\n",
    "\n",
    "- __init__(self, cfg): Initializes the LogOperation object with the provided configuration.\n",
    "\n",
    "- **process(self, dataframe, drop_columns)**: This method executes the data processing operation based on the configuration flag (self.cfg.log_operation).\n",
    "\n",
    "> ### Parameters\n",
    "\n",
    "- **dataframe**: The DataFrame object to be processed.\n",
    "- **drop_columns**: Columns to be dropped from the DataFrame.\n",
    "\n",
    "> ### Processing Steps\n",
    "\n",
    "If self.cfg.log_operation is True:\n",
    "\n",
    "- Copies the DataFrame (dataframe) to a new variable (data).\n",
    "- Drops specified columns (drop_columns) from the copied DataFrame.\n",
    "- Replaces any occurrences of zero (0) with a small positive value (0.001).\n",
    "- Applies a logarithmic transformation (np.log()) to the DataFrame.\n",
    "\n",
    "If self.cfg.log_operation is False:\n",
    "\n",
    "- Copies the DataFrame (dataframe) to a new variable (data).\n",
    "- Drops specified columns (drop_columns) from the copied DataFrame.\n",
    "\n",
    "> ### Returns\n",
    "\n",
    "The processed DataFrame (data), which has undergone either logarithmic transformation or column dropping based on the configuration.\n",
    "\n",
    "> ### Purpose\n",
    "\n",
    "This class provides a data processing operation that conditionally applies a logarithmic transformation to specified columns of a DataFrame if the configuration parameter self.cfg.log_operation is set to True. If False, it drops specific columns from the DataFrame.\n",
    "\n",
    "The LogOperation class offers a way to apply conditional data transformations or column drops to a DataFrame based on the configuration parameter, allowing flexibility in preprocessing based on the specified conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogOperation:\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg  # Initialize LogOperation class with a configuration parameter\n",
    "\n",
    "    def process(self, dataframe, drop_columns):\n",
    "        # Check if log operation is enabled in the configuration\n",
    "        if self.cfg.log_operation:\n",
    "            # If enabled, create a copy of the dataframe and perform log transformation on selected columns\n",
    "            data = dataframe.copy()\n",
    "            data = data.drop(drop_columns, axis=1)  # Drop specified columns\n",
    "            data = data.replace(0, 0.001)  # Replace zeros with a small value to avoid log(0) error\n",
    "            data = np.log(data)  # Perform log transformation on the data\n",
    "\n",
    "        else:\n",
    "            # If log operation is not enabled, create a copy of the dataframe and drop specified columns\n",
    "            data = dataframe.copy()\n",
    "            data = data.drop(drop_columns, axis=1)  # Drop specified columns\n",
    "\n",
    "        return data  # Return the processed data (with or without log transformation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Missing Method**\n",
    "\n",
    "This function is designed to visualize and analyze missing values within a DataFrame by creating a horizontal bar chart that illustrates the percentage of null and non-null values for each column.\n",
    "\n",
    "> ### Input Parameters\n",
    "\n",
    "- **dataframe_name**: Represents the name or identifier of the dataset being analyzed.\n",
    "- **dataframe**: Refers to the DataFrame object containing the dataset.\n",
    "\n",
    "> ### Functionality\n",
    "\n",
    "- **Calculate Missing Values**: Computes the count of null values for each column in the DataFrame using dataframe.isnull().sum().\n",
    "- **Compute Percentages**: Calculates the percentage of null and non-null values for each column by dividing the count of null values by the total length of the DataFrame.\n",
    "- **Prepare Visualization**: Constructs a horizontal bar chart using Matplotlib to visually represent the percentage of null and non-null values for each column.\n",
    "- **Styling**: Applies distinctive colors (red for null values, orange for non-null values) to the bars for clear differentiation.\n",
    "- **Annotations**: Adds annotations displaying the percentage values on the bars to enhance readability.\n",
    "- **Display Chart**: Shows the created bar chart, representing the distribution of missing and non-missing values for each column.\n",
    "Output:\n",
    "\n",
    "An informative bar chart illustrating the percentage of null and non-null values for each column in the dataset.\n",
    "\n",
    "> ### Purpose\n",
    "\n",
    "This function aids in visually assessing the extent of missing values across different columns of a dataset, facilitating quick insights into data completeness and potential imputation needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_values(dataframe_name, dataframe):\n",
    "    # Calculate the count and percentage of missing values in the dataframe\n",
    "    df_null_values = dataframe.isnull().sum().to_frame().rename(columns={0: 'Count'})\n",
    "    df_null_values['Percentage_nulls'] = (df_null_values['Count'] / len(dataframe)) * 100\n",
    "    df_null_values['Percentage_no_nulls'] = 100 - df_null_values['Percentage_nulls']\n",
    "\n",
    "    n = len(df_null_values.index)\n",
    "    x = np.arange(n)\n",
    "\n",
    "    # Create a horizontal bar plot to visualize null and non-null percentages\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    bar_width = 0.4\n",
    "    gap = 0.2\n",
    "\n",
    "    rects1 = ax.barh(x - gap / 2, df_null_values['Percentage_nulls'], bar_width, label='Null values', color='red')\n",
    "    rects2 = ax.barh(x + gap / 2, df_null_values['Percentage_no_nulls'], bar_width, label='No null values', color='orange')\n",
    "\n",
    "    # Set plot properties and labels\n",
    "    ax.set_title(f'{dataframe_name} Null Values and Non-null Values', fontsize=15, fontweight='bold')\n",
    "    ax.set_xlabel('% Percentage', fontsize=12, fontweight='bold')\n",
    "    ax.set_yticks(x)\n",
    "    ax.set_yticklabels(df_null_values.index, fontsize=10, fontweight='bold')\n",
    "\n",
    "    # Hide the top and right spines of the plot, add legend, and label bars with percentages\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.legend()\n",
    "\n",
    "    def autolabel(rects):\n",
    "        # Function to label bars with their respective percentages\n",
    "        for rect in rects:\n",
    "            width = rect.get_width()\n",
    "            ax.annotate(f'{width:.2f}%',\n",
    "                        xy=(width, rect.get_y() + rect.get_height() / 2),\n",
    "                        xytext=(2, 0),\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='left', va='center', size=10, weight='bold')\n",
    "\n",
    "    autolabel(rects1)\n",
    "    autolabel(rects2)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Correalation Matrix Method**\n",
    "\n",
    "This function generates a heatmap representing the correlation matrix among numeric columns in a DataFrame, offering insights into the relationships and dependencies between these columns.\n",
    "\n",
    "> ### Input Parameters\n",
    "\n",
    "- **dataframe_name**: Represents the name or identifier of the dataset being analyzed.\n",
    "- **dataframe**: Refers to the DataFrame object containing the dataset.\n",
    "\n",
    "> ### Functionality\n",
    "\n",
    "- **Select Numeric Columns**: Identifies and selects the numeric columns (both int and float types) from the DataFrame using select_dtypes() method and include=[int, float].\n",
    "- **Extract Relevant Data**: Extracts the numerical data columns from the DataFrame based on the identified numeric columns.\n",
    "- **Calculate Correlation**: Computes the correlation matrix of the extracted numerical data using the corr() method, measuring the pairwise correlations between columns.\n",
    "- **Create Heatmap**: Constructs a heatmap using Seaborn (sns.heatmap()) to visualize the correlation matrix with annotations (annot=True), color spectrum (cmap='coolwarm'), formatting to three decimal places (fmt=\".3f\"), and specified linewidths.\n",
    "- **Title and Display**: Sets the title of the heatmap based on the provided dataframe_name and displays the generated heatmap.\n",
    "\n",
    "> ### Output\n",
    "\n",
    "A heatmap visualizing the correlation matrix among numeric columns in the dataset, showing the strength and direction of correlations between variables.\n",
    "\n",
    "> ### Purpose\n",
    "\n",
    "This function aids in identifying relationships and patterns among numeric features within the dataset, assisting in feature selection, identifying multicollinearity, and understanding interdependencies between variables.\n",
    "\n",
    "The function enables a quick visualization of the correlation structure between numeric columns, providing insights into the strength and direction of relationships within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_matrix(dataframe_name, dataframe):\n",
    "    # Select numerical columns from the dataframe\n",
    "    num_cols = dataframe.select_dtypes(include=[int, float]).columns\n",
    "\n",
    "    # Get the numerical data based on selected columns\n",
    "    variables = num_cols\n",
    "    data = dataframe[variables]\n",
    "\n",
    "    # Compute the correlation matrix\n",
    "    correlation_matrix = data.corr()\n",
    "\n",
    "    # Plot the correlation matrix as a heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".3f\", linewidths=1)\n",
    "    plt.title(f'{dataframe_name} Correlation Matrix')\n",
    "    plt.show()\n",
    "    print(\"\\n\\n\")  # Add an empty line for better separation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **About Columns**\n",
    "\n",
    "This function is designed to categorize columns within a DataFrame based on their data types and unique value counts, aiding in feature categorization and differentiation.\n",
    "\n",
    "> ### Input Parameters\n",
    "\n",
    "- **dataframe_name**: Represents the name or identifier of the dataset being analyzed.\n",
    "- **dataframe**: Refers to the DataFrame object containing the dataset.\n",
    "- **cat_th**: Threshold value to determine categorical columns based on their unique value counts (default set to 10).\n",
    "- **car_th**: Threshold value to define categorical but cardinal columns (default set to 20).\n",
    "- **print_results**: A Boolean parameter controlling the printing of results (default set to True).\n",
    "\n",
    "> ### Categorize Columns\n",
    "\n",
    "- **cat_cols**: Identifies columns with data types of \"category,\" \"object,\" or \"bool\" using list comprehension.\n",
    "- **num_but_cat**: Selects columns with less than cat_th unique values and having data types of \"int64\" or \"float64\".\n",
    "- **cat_but_car**: Finds columns with more than car_th unique values and data types of \"category\" or \"object\".\n",
    "\n",
    "> ### Create Categorical and Numerical Lists\n",
    "\n",
    "- **cat_cols**: Removes columns in cat_but_car from cat_cols and appends columns from num_but_cat.\n",
    "- **num_cols**: Selects columns with data types of \"int64\" or \"float64\" that are not in cat_cols.\n",
    "\n",
    "> ### Print Summary (Optional)\n",
    "If print_results is True, displays a summary containing dataset information, such as the number of observations, variables, categorical columns (cat_cols), numerical columns (num_cols), and additional categorized columns.\n",
    "\n",
    "> ### Return\n",
    "Returns a list of numerical columns (num_cols).\n",
    "\n",
    "> ### Output\n",
    "\n",
    "Optionally prints a summary and returns a list of numerical columns.\n",
    "\n",
    "> ### Purpose\n",
    "\n",
    "Facilitates the identification and categorization of columns into numerical and categorical types within the dataset, assisting in subsequent analysis, preprocessing, or modeling tasks.\n",
    "\n",
    "This function is useful for classifying columns based on their data types and unique value counts, aiding in the understanding and handling of different types of features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_col_names(dataframe_name, dataframe, cat_th=10, car_th=20, print_results=True):\n",
    "    # Identify categorical columns based on data type and unique value counts\n",
    "    cat_cols = [col for col in dataframe.columns if str(dataframe[col].dtypes) in [\"category\", \"object\", \"bool\"]]\n",
    "    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and dataframe[col].dtypes in [\"int64\", \"float64\"]]\n",
    "    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and str(dataframe[col].dtypes) in [\"category\", \"object\"]]\n",
    "    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n",
    "    cat_cols = cat_cols + num_but_cat\n",
    "    \n",
    "    # Identify numerical columns excluding previously identified categorical columns\n",
    "    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes in [\"int64\", \"float64\"]]\n",
    "    num_cols = [col for col in num_cols if col not in cat_cols]\n",
    "\n",
    "    # Print results if specified\n",
    "    if print_results:\n",
    "        print(f\"{dataframe_name} Dataset\")\n",
    "        print(\"*\"*20, \"\\n\")\n",
    "        print(f'Observations {dataframe.shape[0]}')\n",
    "        print(f'Variables:  {dataframe.shape[1]}')\n",
    "        print(f'cat_cols:  {len(cat_cols)}')\n",
    "        print(f'num_cols:  {len(num_cols)}')\n",
    "        print(f'cat_but_car:  {len(cat_but_car)}')\n",
    "        print(f'num_but_cat:  {len(num_but_cat)}\\n\\n')\n",
    "\n",
    "    return num_cols  # Return the list of numerical columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BoxPlot Method**\n",
    "\n",
    "This function is used to create a grid of box plots for numerical columns in a dataset, aiding in the visualization of their distributions and statistical characteristics.\n",
    "\n",
    "> ### Input Parameters\n",
    "\n",
    "- **dataset_name**: Represents the name or identifier of the dataset being visualized.\n",
    "- **dataframe**: Refers to the DataFrame object containing the dataset.\n",
    "- **num_cols**: A list of numerical columns from the dataset to be visualized using box plots.\n",
    "- **ncols**: Number of columns in the grid layout (default set to 2).\n",
    "\n",
    "> ### Functionality\n",
    "\n",
    "Calculates the number of required rows (nrows) based on the number of numerical columns and the specified ncols.\n",
    "\n",
    "> ### Create Subplots\n",
    "\n",
    "Generates a grid of subplots (nrows x ncols) with a specified figure size.\n",
    "\n",
    "> ### Plot Box Plots\n",
    "\n",
    "- Iterates through each numerical column in num_cols.\n",
    "- Checks if the column exists in the DataFrame and creates a box plot for the column using Seaborn's boxplot() function.\n",
    "- Each box plot represents the distribution of values for a specific numerical column.\n",
    "- Assigns titles to each subplot based on the column name.\n",
    "\n",
    "> ### Adjust Layout and Display\n",
    "\n",
    "Adjusts the layout of subplots to prevent overlapping.\n",
    "Displays the grid of box plots.\n",
    "\n",
    "> ### Output\n",
    "\n",
    "Displays a grid of box plots for the specified numerical columns in the dataset.\n",
    "\n",
    "> ### Purpose\n",
    "\n",
    "Facilitates a visual examination of the distribution, central tendency, and spread of numerical data in the dataset through box plots, aiding in identifying potential outliers and understanding data variability.\n",
    "\n",
    "This function offers an efficient way to visualize multiple numerical columns simultaneously, allowing for a quick overview of their distributions and statistical summaries using box plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplot(dataset_name, dataframe, num_cols, ncols=2):\n",
    "    # Calculate the number of rows needed for subplots based on the number of numerical columns and ncols\n",
    "    nrows = (len(num_cols) + ncols - 1) // ncols\n",
    "    \n",
    "    # Create subplots based on nrows and ncols\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(10, 3*nrows))\n",
    "    fig.suptitle(f\"{dataset_name} Visualization\")  # Set the main title of the visualization\n",
    "\n",
    "    colors = sns.color_palette('Set3', n_colors=len(dataframe.columns))  # Define colors for each boxplot\n",
    "    \n",
    "    # Plot boxplots for each numerical column in the dataframe\n",
    "    for i, col in enumerate(num_cols):\n",
    "        if col in dataframe.columns:\n",
    "            ax = axes[i // ncols, i % ncols]  # Define the axes for each subplot\n",
    "            sns.boxplot(x=dataframe[col], ax=ax, color=colors[i])  # Create the boxplot\n",
    "            ax.set_title(f\"Box Plot of {col}\")  # Set the title for each subplot\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout for better visualization\n",
    "    plt.show()  # Display the plot\n",
    "    print(\"\\n\\n\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **HistPlot Method**\n",
    "\n",
    "This function is designed to generate a grid layout of histograms for numerical columns in a dataset, providing visual insights into their distributions.\n",
    "\n",
    "> ### Input Parameters\n",
    "\n",
    "- **dataframe_name**: Represents the name or identifier of the dataset being visualized.\n",
    "- **dataframe**: Refers to the DataFrame object containing the dataset.\n",
    "- **num_cols**: A list of numerical columns from the dataset to be visualized using histograms.\n",
    "- **ncols**: Number of columns in the grid layout (default set to 2).\n",
    "- **bins**: Number of bins for the histograms (default set to 10).\n",
    "\n",
    "> ### Functionality\n",
    "\n",
    "Calculates the number of required rows (nrows) based on the number of numerical columns and the specified ncols.\n",
    "\n",
    "> ### Create Subplots\n",
    "\n",
    "Generates a grid of subplots (nrows x ncols) with a specified figure size.\n",
    "\n",
    "> ### Plot Histograms\n",
    "\n",
    "- Iterates through each numerical column in num_cols.\n",
    "- Checks if the column exists in the DataFrame and creates a histogram for the column using Seaborn's histplot() function.\n",
    "- Each histogram represents the distribution of values for a specific numerical column.\n",
    "- Assigns titles to each subplot based on the column name.\n",
    "\n",
    "> ### Adjust Layout and Display\n",
    "\n",
    "Adjusts the layout of subplots to prevent overlapping.\n",
    "Displays the grid of histograms.\n",
    "\n",
    "> ### Output\n",
    "\n",
    "Displays a grid of histograms for the specified numerical columns in the dataset.\n",
    "\n",
    "> ### Purpose\n",
    "\n",
    "Provides an overview of the distribution patterns of numerical data in the dataset through histograms, enabling insights into the data's central tendency, spread, and potential skewness or outliers.\n",
    "\n",
    "This function offers a convenient way to visualize the distribution of multiple numerical columns simultaneously by creating a grid of histograms. It allows for a quick examination of the distribution characteristics of each numerical variable in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_plot(dataframe_name, dataframe, num_cols, ncols=2, bins=10):\n",
    "    # Calculate the number of rows needed for subplots based on the number of numerical columns and ncols\n",
    "    nrows = (len(num_cols) + ncols - 1) // ncols\n",
    "    \n",
    "    # Create subplots based on nrows and ncols\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(10, 3*nrows))\n",
    "    fig.suptitle(f'{dataframe_name} Histogram Visualization')  # Set the main title of the visualization\n",
    "\n",
    "    # Plot histograms for each numerical column in the dataframe\n",
    "    for i, col in enumerate(num_cols):\n",
    "        if col in dataframe.columns:\n",
    "            ax = axes[i // ncols, i % ncols]  # Define the axes for each subplot\n",
    "            sns.histplot(dataframe[col], ax=ax, bins=bins, kde=True)  # Create the histogram plot\n",
    "            ax.set_title(f\"Histogram of {col}\")  # Set the title for each subplot\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout for better visualization\n",
    "    plt.show()  # Display the plot\n",
    "    print(\"\\n\\n\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Check Outlier Method**\n",
    "\n",
    "This function checks for potential outliers within numerical columns in a dataset using the interquartile range (IQR) method.\n",
    "\n",
    "> ### Input Parameters\n",
    "\n",
    "**dataframe_name**: Name or identifier of the dataset being checked for outliers.\n",
    "**dataframe**: DataFrame object containing the dataset.\n",
    "**num_cols**: List of numerical columns in the dataset to be checked for outliers.\n",
    "**low_quantile**: Lower quantile threshold (default set to 0.10, representing 10th percentile).\n",
    "**up_quantile**: Upper quantile threshold (default set to 0.90, representing 90th percentile).\n",
    "\n",
    "> ### Functionality\n",
    "\n",
    "Loops through each numerical column specified in num_cols.\n",
    "\n",
    "> ### Outlier Detection\n",
    "\n",
    "- Computes the first quartile (quantile_one) and third quartile (quantile_three) using the specified quantiles.\n",
    "- Calculates the interquartile range (interquantile_range) as the difference between the third and first quartiles.\n",
    "- Determines upper and lower limits for potential outliers using the IQR method.\n",
    "- Checks if any values in the column exceed the defined outlier limits.\n",
    "\n",
    "> ### Output Display\n",
    "\n",
    "Prints a message for each column indicating the presence or absence of outliers based on the defined limits.\n",
    "\n",
    "> ### Purpose\n",
    "\n",
    "- Aims to identify potential outliers within numerical columns by analyzing their positions relative to the IQR boundaries.\n",
    "- Offers an insight into columns where values significantly deviate from the central data distribution.\n",
    "\n",
    "This function serves as a basic outlier detection method by utilizing the IQR technique to flag potential outliers in the numerical columns of a dataset. It helps in identifying data points that may require further investigation due to their extreme values in comparison to the rest of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_outlier(dataframe_name, dataframe, num_cols, low_quantile=0.10, up_quantile=0.90):\n",
    "    # Check for outliers in numerical columns within the specified quantiles\n",
    "    print(f'\\n Checking for {dataframe_name} Dataset')\n",
    "    for col in num_cols:\n",
    "        quantile_one = dataframe[col].quantile(low_quantile)  # Get the lower quantile\n",
    "        quantile_three = dataframe[col].quantile(up_quantile)  # Get the upper quantile\n",
    "\n",
    "        interquantile_range = quantile_three - quantile_one  # Calculate the interquartile range\n",
    "        up_limit = quantile_three + 1.5 * interquantile_range  # Calculate the upper limit\n",
    "        low_limit = quantile_one - 1.5 * interquantile_range  # Calculate the lower limit\n",
    "\n",
    "        # Check if the column has outliers based on the limits\n",
    "        if dataframe[(dataframe[col] > up_limit) | (dataframe[col] < low_limit)].any(axis=None):\n",
    "            print(f'{col} column has outliers..')\n",
    "        else:\n",
    "            print(f'{col} column has no outliers..')\n",
    "\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Outlier Handling Method**\n",
    "\n",
    "This class provides methods for outlier detection in a dataset using Isolation Forest (IForest) or Local Outlier Factor (LOF) algorithms.\n",
    "\n",
    "> ### Constructor (__init__)\n",
    "\n",
    "Initializes the class instance with configuration settings (cfg) used to determine the outlier detection method to apply.\n",
    "\n",
    "\n",
    "> ### Input Parameters\n",
    "\n",
    "- **dataframe_name**: Name or identifier of the dataset.\n",
    "- **dataframe**: DataFrame object containing the dataset.\n",
    "- **num_cols**: List of numerical columns in the dataset to perform outlier detection.\n",
    "- **contamination**: The proportion of outliers to expect in the dataset.\n",
    "- **n_neighbors**: Number of neighbors to consider (for LOF method).\n",
    "\n",
    "> ### IForest (Isolation Forest)\n",
    "\n",
    "- Applies Isolation Forest algorithm if cfg.IForest is set to True.\n",
    "- Detects outliers using Isolation Forest algorithm.\n",
    "- Marks anomalies in the dataset by assigning 1 to an 'anomaly' column for detected outliers.\n",
    "- Prints a message indicating the application of Isolation Forest for the specified dataset.\n",
    "\n",
    "> ### LOF (Local Outlier Factor)\n",
    "\n",
    "- Applies Local Outlier Factor algorithm if cfg.LOF is set to True.\n",
    "- Detects outliers using Local Outlier Factor algorithm.\n",
    "- Marks anomalies in the dataset by assigning 1 to an 'anomaly' column for detected outliers.\n",
    "- Prints a message indicating the application of LOF for the specified dataset.\n",
    "\n",
    "> ### Return\n",
    "\n",
    "Returns the modified DataFrame with an additional 'anomaly' column indicating outlier presence or the original DataFrame if outlier detection is disabled.\n",
    "\n",
    "> ### Purpose\n",
    "\n",
    "- Facilitates outlier detection by applying specific algorithms based on the provided configuration settings.\n",
    "I- dentifies anomalies in the dataset to highlight potential data points that deviate significantly from the general pattern.\n",
    "\n",
    "This class enables the automatic detection of outliers using Isolation Forest or Local Outlier Factor techniques based on the user-defined configurations, providing a convenient way to identify potential anomalies within a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlierDetection:\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def process(self, dataframe_name, dataframe, num_cols, contamination, n_neighbors):\n",
    "        # Apply outlier detection algorithms based on configuration settings\n",
    "        if self.cfg.IForest:  # Check if Isolation Forest is enabled\n",
    "            # Isolation Forest algorithm implementation\n",
    "            X = dataframe[num_cols]\n",
    "            model = IsolationForest(n_estimators=len(dataframe), contamination=contamination)\n",
    "            model.fit(X)\n",
    "            anomaly_label = model.predict(X)\n",
    "            anomaly = X[anomaly_label == -1]\n",
    "            # Mark anomalies in the dataset\n",
    "            dataframe['anomaly'] = 0\n",
    "            dataframe.loc[anomaly.index, 'anomaly'] = 1\n",
    "            print(f'Isolation Forest applied for {dataframe_name} dataset..')\n",
    "        \n",
    "        elif self.cfg.LOF:  # Check if Local Outlier Factor is enabled\n",
    "            # Local Outlier Factor algorithm implementation\n",
    "            X = dataframe[num_cols]\n",
    "            lof_model = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=contamination)\n",
    "            outlier_labels = lof_model.fit_predict(X)\n",
    "            anomaly = X[outlier_labels == -1]\n",
    "            # Mark anomalies in the dataset\n",
    "            dataframe['anomaly'] = 0\n",
    "            dataframe.loc[anomaly.index, 'anomaly'] = 1\n",
    "            print(f'Local Outlier Factor applied for {dataframe_name} dataset..')\n",
    "        \n",
    "        else:\n",
    "            print('Outlier Detection process is disabled. Returning original dataframe.')\n",
    "            \n",
    "        return dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Feature Engineering (for wind_plant)**\n",
    "\n",
    "This class provides a method for extracting new features from a dataset.\n",
    "\n",
    "> ### Constructor (__init__)\n",
    "\n",
    "Initializes the class instance with configuration settings (cfg) used to determine whether to extract features.\n",
    "\n",
    "> ### Input Parameters\n",
    "\n",
    "- **dataframe_name**: Name or identifier of the dataset.\n",
    "- **dataframe**: DataFrame object containing the dataset.\n",
    "- **target_col**: The column considered as the target variable.\n",
    "\n",
    "### Conditional Feature Extraction\n",
    "\n",
    "- Derives additional features from the dataset based on numerical columns.\n",
    "- Computes statistical features like mean, standard deviation, and variance across rows.\n",
    "- Applies trigonometric functions to specific columns ('windDir10m' and 'windDir100m').\n",
    "- Calculates rolling mean for 'windSpeed10m' and 'windSpeed100m' columns.\n",
    "\n",
    "> ### Return\n",
    "\n",
    "Returns the modified DataFrame with added features or the original DataFrame if feature extraction is disabled.\n",
    "\n",
    "> ### Purpose\n",
    "\n",
    "- Facilitates the creation of new derived features from the dataset, such as statistical aggregates and trigonometric transformations.\n",
    "- Enhances the dataset with additional information that might improve model performance during training or analysis.\n",
    "\n",
    "This class enables the extraction of supplementary features from the dataset based on user-defined conditions, providing an enriched dataset with new information that might be beneficial for modeling or analysis purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractFeature:\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def process(self, dataframe_name, dataframe, target_col):\n",
    "        # Check if feature extraction is enabled in the configuration\n",
    "        if self.cfg.extract_features:\n",
    "            # Select numerical columns excluding the target column\n",
    "            num_cols = dataframe.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "            num_cols = [col for col in num_cols if col != target_col]\n",
    "\n",
    "            # Perform feature extraction\n",
    "            dataframe['mean'] = dataframe[num_cols].mean(axis=1)\n",
    "            dataframe['std'] = dataframe[num_cols].std(axis=1)\n",
    "            dataframe['var'] = dataframe[num_cols].var(axis=1)\n",
    "            dataframe['windir10m_sin'] = np.sin(np.deg2rad(dataframe['windDir10m']))\n",
    "            dataframe['windDir10m_cos'] = np.cos(np.deg2rad(dataframe['windDir10m']))\n",
    "            dataframe['windir100m_sin'] = np.sin(np.deg2rad(dataframe['windDir100m']))\n",
    "            dataframe['windDir100m_cos'] = np.cos(np.deg2rad(dataframe['windDir100m']))\n",
    "            dataframe['windSpeed10m_hourly_mean'] = dataframe['windSpeed10m'].rolling(window=1).mean()\n",
    "            dataframe['windSpeed100m_hourly_mean'] = dataframe['windSpeed100m'].rolling(window=1).mean()\n",
    "\n",
    "            print(f\"All features extracted for {dataframe_name} dataset..\")\n",
    "            \n",
    "        else:\n",
    "            print(\"Extract features is disabled. Returning original dataframe.\")\n",
    "        \n",
    "        return dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **About Datetime**\n",
    "\n",
    "This function is used to process a DataFrame's date-related column to derive additional temporal features.\n",
    "\n",
    "> ### Input Parameters\n",
    "\n",
    "- **dataframe**: DataFrame containing the date-related information.\n",
    "- **date_column**: Column name representing the date or timestamp.\n",
    "\n",
    "> ### Date Processing Steps\n",
    "\n",
    "- Converts the specified date_column to a Pandas datetime object.\n",
    "- Sets the datetime column as the index for the DataFrame.\n",
    "- Sorts the DataFrame based on the datetime index.\n",
    "- **year**: Extracts the year.\n",
    "- **quarter**: Extracts the quarter of the year.\n",
    "- **month**: Extracts the month.\n",
    "- **week**: Extracts the week of the year using ISO week numbering.\n",
    "- **day_of_week**: Extracts the day of the week (0 = Monday, 6 = Sunday).\n",
    "- **day_of_year**: Extracts the day of the year.\n",
    "- **is_weekend**: Flags whether the day falls on a weekend (Saturday or Sunday).\n",
    "- **hour**: Extracts the hour of the day.\n",
    "\n",
    "> ### Return\n",
    "\n",
    "Returns the modified DataFrame with added temporal features based on the date_column.\n",
    "\n",
    "> ### Purpose\n",
    "\n",
    "- Facilitates the extraction of various temporal attributes from a date column.\n",
    "- Enriches the dataset with time-related information that could be useful for time-series analysis or modeling.\n",
    "\n",
    "This function enables the creation of a more detailed dataset by incorporating temporal information extracted from the specified date column, allowing for better analysis or modeling, especially in time-oriented datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime_process(dataframe, date_column):\n",
    "    # Convert the specified column to datetime format\n",
    "    dataframe[date_column] = pd.to_datetime(dataframe[date_column])\n",
    "    \n",
    "    # Set the datetime column as the index\n",
    "    dataframe = dataframe.set_index(date_column, drop=True)\n",
    "    \n",
    "    # Sort the dataframe based on the datetime index\n",
    "    dataframe.sort_index(inplace=True)\n",
    "    \n",
    "    # Extract various date and time-related features\n",
    "    dataframe['year'] = dataframe.index.year\n",
    "    dataframe['quarter'] = dataframe.index.quarter\n",
    "    dataframe['month'] = dataframe.index.month\n",
    "    dataframe['week'] = dataframe.index.isocalendar().week\n",
    "    dataframe['day_of_week'] = dataframe.index.dayofweek\n",
    "    dataframe['day_of_year'] = dataframe.index.dayofyear\n",
    "    dataframe['is_weekend'] = dataframe.index.dayofweek.isin([5, 6]).astype(int)\n",
    "    dataframe['hour'] = dataframe.index.hour\n",
    "\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig_specs(fig1, fig2, dataframe, year_col, x_col, y_col):\n",
    "    # Set visibility of initial traces to False\n",
    "    fig1.data[0].visible = False\n",
    "    fig2.data[0].visible = False\n",
    "\n",
    "    # Create buttons to toggle visibility of traces for different years\n",
    "    buttons = [{'label': 'Choose Option', 'method': 'update', 'args': [{'visible': [False] * len(fig1.data)}]}]\n",
    "\n",
    "    for year in dataframe[year_col].unique():\n",
    "        button = {'label': str(year), 'method': 'update',\n",
    "                  'args': [{'visible': [trace.name.startswith(str(year)) for trace in fig1.data]}]}\n",
    "        buttons.append(button)\n",
    "\n",
    "    # Create a menu specification dictionary for the interactive buttons\n",
    "    menu_spec_dict = {'buttons': buttons, 'direction': 'down', 'showactive': True,\n",
    "                      'x': 0.5, 'xanchor': 'center', 'y': 1.15, 'yanchor': 'top'}\n",
    "\n",
    "    # Update layout settings and titles for both figures\n",
    "    fig1.update_layout(updatemenus=[menu_spec_dict],\n",
    "                       title=f\"{x_col}ly Total {y_col} Usage by Year\",\n",
    "                       xaxis_title=x_col, yaxis_title=y_col, barmode='stack')\n",
    "\n",
    "    fig2.update_layout(updatemenus=[menu_spec_dict],\n",
    "                       title=f'{x_col}ly Mean {y_col} Usage by Year',\n",
    "                       xaxis_title=x_col, yaxis_title=y_col, barmode='stack')\n",
    "\n",
    "    # Show the figures with interactive controls\n",
    "    fig1.show()\n",
    "    fig2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_analysis_quarterly(dataframe, year_col, quarter_col, target_col):\n",
    "    # Initialize figures for total and mean analyses\n",
    "    fig_total = go.Figure()\n",
    "    fig_mean = go.Figure()\n",
    "\n",
    "    # Define color scheme for each year\n",
    "    colors = px.colors.qualitative.Set2\n",
    "\n",
    "    # Loop through unique years in the dataset\n",
    "    for idx, year in enumerate(dataframe[year_col].unique()):\n",
    "        # Extract data for the current year\n",
    "        data_of_yearly = dataframe[dataframe[year_col] == year]\n",
    "        \n",
    "        # Calculate total demand per quarter for the current year\n",
    "        weekly_total_demand = data_of_yearly.groupby(quarter_col)[target_col].sum().reset_index()\n",
    "        # Add a bar trace for total demand to the total figure\n",
    "        fig_total.add_trace(go.Bar(x=weekly_total_demand[quarter_col], \n",
    "                                   y=weekly_total_demand[target_col], \n",
    "                                   name=str(year),\n",
    "                                   visible=False,\n",
    "                                   marker_color=colors[idx]))\n",
    "\n",
    "        # Calculate mean demand per quarter for the current year\n",
    "        weekly_mean_demand = data_of_yearly.groupby(quarter_col)[target_col].mean().reset_index()\n",
    "        # Add a bar trace for mean demand to the mean figure\n",
    "        fig_mean.add_trace(go.Bar(x=weekly_mean_demand[quarter_col], \n",
    "                                  y=weekly_mean_demand[target_col], \n",
    "                                  name=f'{year}', \n",
    "                                  marker_color=colors[idx],\n",
    "                                  visible=False, \n",
    "                                  opacity=0.5))\n",
    "\n",
    "    # Use the fig_specs function to display both figures with interactive controls\n",
    "    fig_specs(fig_total, fig_mean, dataframe, year_col, quarter_col, target_col)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_analysis_monthly(dataframe, year_col, month_col, target_col):\n",
    "    # Initialize figures for total and mean analyses\n",
    "    fig_total = go.Figure()\n",
    "    fig_mean = go.Figure()\n",
    "\n",
    "    # Define color scheme for each year\n",
    "    colors = px.colors.qualitative.Vivid\n",
    "\n",
    "    # Loop through unique years in the dataset\n",
    "    for idx, year in enumerate(dataframe[year_col].unique()):\n",
    "        # Extract data for the current year\n",
    "        data_of_yearly = dataframe[dataframe[year_col] == year]\n",
    "        \n",
    "        # Calculate total demand per month for the current year\n",
    "        monthly_total_demand = data_of_yearly.groupby(month_col)[target_col].sum().reset_index()\n",
    "        # Add a bar trace for total demand to the total figure\n",
    "        fig_total.add_trace(go.Bar(x=monthly_total_demand[month_col], \n",
    "                                   y=monthly_total_demand[target_col], \n",
    "                                   name=str(year),\n",
    "                                   visible=False,\n",
    "                                   marker_color=colors[idx]))\n",
    "\n",
    "        # Calculate mean demand per month for the current year\n",
    "        monthly_mean_demand = data_of_yearly.groupby(month_col)[target_col].mean().reset_index()\n",
    "        # Add a bar trace for mean demand to the mean figure\n",
    "        fig_mean.add_trace(go.Bar(x=monthly_mean_demand[month_col], \n",
    "                                  y=monthly_mean_demand[target_col], \n",
    "                                  name=f'{year}', \n",
    "                                  marker_color=colors[idx],\n",
    "                                  visible=False, \n",
    "                                  opacity=0.5))\n",
    "\n",
    "    # Use the fig_specs function to display both figures with interactive controls\n",
    "    fig_specs(fig_total, fig_mean, dataframe, year_col, month_col, target_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_analysis_weekly(dataframe, year_col, week_col, target_col):\n",
    "    # Initialize figures for total and mean analyses\n",
    "    fig_total = go.Figure()\n",
    "    fig_mean = go.Figure()\n",
    "\n",
    "    # Define color scheme for each year\n",
    "    colors = px.colors.qualitative.Vivid\n",
    "\n",
    "    # Loop through unique years in the dataset\n",
    "    for idx, year in enumerate(dataframe[year_col].unique()):\n",
    "        # Extract data for the current year\n",
    "        data_of_yearly = dataframe[dataframe[year_col] == year]\n",
    "        \n",
    "        # Calculate total demand per week for the current year\n",
    "        weekly_total_demand = data_of_yearly.groupby(week_col)[target_col].sum().reset_index()\n",
    "        # Add a bar trace for total demand to the total figure\n",
    "        fig_total.add_trace(go.Bar(x=weekly_total_demand[week_col], \n",
    "                                   y=weekly_total_demand[target_col], \n",
    "                                   name=str(year),\n",
    "                                   visible=False,\n",
    "                                   marker_color=colors[idx]))\n",
    "\n",
    "        # Calculate mean demand per week for the current year\n",
    "        weekly_mean_demand = data_of_yearly.groupby(week_col)[target_col].mean().reset_index()\n",
    "        # Add a bar trace for mean demand to the mean figure\n",
    "        fig_mean.add_trace(go.Bar(x=weekly_mean_demand[week_col], \n",
    "                                  y=weekly_mean_demand[target_col], \n",
    "                                  name=f'{year}', \n",
    "                                  marker_color=colors[idx],\n",
    "                                  visible=False, \n",
    "                                  opacity=0.5))\n",
    "\n",
    "    # Use the fig_specs function to display both figures with interactive controls\n",
    "    fig_specs(fig_total, fig_mean, dataframe, year_col, week_col, target_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_analysis_hourly(dataframe, year_col, month_col, hour_col, target_col):\n",
    "    # Group data by year, month, and hour to calculate hourly usage and mean\n",
    "    hourly_usage = dataframe.groupby([year_col, month_col, hour_col])[target_col].sum().reset_index()\n",
    "    hourly_mean = dataframe.groupby([year_col, month_col, hour_col])[target_col].mean().reset_index()\n",
    "\n",
    "    # Initialize figures for total and mean analyses\n",
    "    fig_total = go.Figure()\n",
    "    fig_mean = go.Figure()\n",
    "\n",
    "    # Loop through unique years in the dataset\n",
    "    for year in dataframe[year_col].unique():\n",
    "        year_data = hourly_usage[hourly_usage[year_col] == year]\n",
    "        year_mean = hourly_mean[hourly_mean[year_col] == year]\n",
    "\n",
    "        # Check if the year's data is available\n",
    "        if not year_data.empty:\n",
    "            for month in year_data[month_col].unique():\n",
    "                monthly_data = year_data[year_data[month_col] == month]\n",
    "                monthly_mean = year_mean[year_mean[month_col] == month]\n",
    "\n",
    "                # Check if both monthly data and mean are available\n",
    "                if not monthly_data.empty and not monthly_mean.empty:\n",
    "                    # Add a bar trace for hourly total usage to the total figure\n",
    "                    fig_total.add_trace(go.Bar(\n",
    "                        x=monthly_data[hour_col],\n",
    "                        y=monthly_data[target_col],\n",
    "                        name=f'{year} - {month}',\n",
    "                        visible=False,\n",
    "                    ))\n",
    "\n",
    "                    # Add a bar trace for hourly mean usage to the mean figure\n",
    "                    fig_mean.add_trace(go.Bar(\n",
    "                        x=monthly_mean[hour_col],\n",
    "                        y=monthly_mean[target_col],\n",
    "                        name=f'{year} - {month}',\n",
    "                        visible=False,\n",
    "                        opacity=0.5\n",
    "                    ))\n",
    "\n",
    "    # Use the fig_specs function to display both figures with interactive controls\n",
    "    fig_specs(fig_total, fig_mean, dataframe, year_col, hour_col, target_col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Scaler Method**\n",
    "\n",
    "This class contains a method to scale numerical columns within DataFrames according to specific scaling techniques.\n",
    "\n",
    "> ### Attributes\n",
    "\n",
    "- **cfg**: Configuration settings that determine the type of scaling to be applied.\n",
    "\n",
    "> ### Inputs\n",
    "\n",
    "- **dataframe**: The training dataset containing numerical columns to be scaled.\n",
    "- **dataframe_test**: The testing dataset with the same numerical columns as the training set.\n",
    "- **week_col**: The column representing weeks, converted to integer type for consistency.\n",
    "\n",
    "> ### MinMaxScaler\n",
    "\n",
    "Scales numerical columns within the DataFrame using MinMaxScaler to a range of 0 to 1.\n",
    "\n",
    "> ### StandardScaler\n",
    "\n",
    "Utilizes StandardScaler to standardize numerical columns with mean=0 and standard deviation=1.\n",
    "\n",
    "> ### RobustScaler\n",
    "\n",
    "Applies RobustScaler, which is robust to outliers by scaling data based on median and interquartile range.\n",
    "\n",
    "> ### Operations\n",
    "\n",
    "- Checks the specified configuration settings to determine the scaling technique to be applied.\n",
    "- Scales numerical columns of the training and testing datasets accordingly.\n",
    "- Prints a message indicating the successful application of the selected scaling process.\n",
    "\n",
    "> ### Returns\n",
    "\n",
    "Modified training and testing DataFrames with scaled numerical columns.\n",
    "\n",
    "> ### Purpose\n",
    "\n",
    "Facilitates the scaling of numerical features in datasets based on specified configurations.\n",
    "Ensures that training and testing datasets undergo the same scaling process for consistency in modeling or analysis.\n",
    "\n",
    "This class allows for the uniform application of scaling techniques to numerical columns in both training and testing datasets, providing consistent and standardized data for modeling or analysis purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scaler:\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def process(self, dataframe, dataframe_test, week_col):\n",
    "        # Convert week_col to integer type\n",
    "        dataframe[week_col] = dataframe[week_col].astype(int)\n",
    "        dataframe_test[week_col] = dataframe_test[week_col].astype(int)\n",
    "        \n",
    "        # Extract numerical columns\n",
    "        num_columns = dataframe.select_dtypes(include=(int, float)).columns.tolist()\n",
    "\n",
    "        if self.cfg.min_max_scaler:\n",
    "            # Apply MinMaxScaler\n",
    "            print(\"MinMaxScaler process worked..\")\n",
    "            mm = MinMaxScaler(feature_range=(0, 1))\n",
    "            scaler = mm.fit(dataframe[num_columns])\n",
    "            dataframe[num_columns] = pd.DataFrame(scaler.transform(dataframe[num_columns]), \n",
    "                                     index=dataframe.index, columns=dataframe.columns)\n",
    "            dataframe_test[num_columns] = pd.DataFrame(scaler.transform(dataframe_test[num_columns]), \n",
    "                                          index=dataframe_test.index, columns=dataframe_test.columns)\n",
    "            \n",
    "            return dataframe, dataframe_test, mm\n",
    "\n",
    "\n",
    "\n",
    "        elif self.cfg.standard_scaler:\n",
    "            # Apply StandardScaler\n",
    "            print(\"StandardScaler process worked..\")\n",
    "            ss = StandardScaler()\n",
    "            scaler = ss.fit(dataframe[num_columns])\n",
    "            dataframe[num_columns] = pd.DataFrame(scaler.transform(dataframe[num_columns]), \n",
    "                                     index=dataframe.index, columns=dataframe.columns)\n",
    "            dataframe_test[num_columns] = pd.DataFrame(scaler.transform(dataframe_test[num_columns]), \n",
    "                                          index=dataframe_test.index, columns=dataframe_test.columns)\n",
    "\n",
    "            return dataframe, dataframe_test, ss\n",
    "        \n",
    "\n",
    "\n",
    "        elif self.cfg.robust_scaler:\n",
    "            # Apply RobustScaler\n",
    "            print(\"RobustScaler process worked..\")\n",
    "            rs = RobustScaler()\n",
    "            scaler = rs.fit(dataframe[num_columns])\n",
    "            dataframe[num_columns] = pd.DataFrame(scaler.transform(dataframe[num_columns]), \n",
    "                                     index=dataframe.index, columns=dataframe.columns)\n",
    "            dataframe_test[num_columns] = pd.DataFrame(scaler.transform(dataframe_test[num_columns]), \n",
    "                                          index=dataframe_test.index, columns=dataframe_test.columns)\n",
    "\n",
    "            return dataframe, dataframe_test, rs\n",
    "        \n",
    "        else:\n",
    "            print('All scaler processes are disabled. Returning original dataframes..')\n",
    "        \n",
    "            return dataframe, dataframe_test, scaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Hyperparameter Tuning Method**\n",
    "\n",
    "This class aids in optimizing hyperparameters for various regression models using the Optuna library.\n",
    "\n",
    "> ### Attributes\n",
    "\n",
    "- **cfg**: Configuration settings determining the type of hyperparameter tuning to be performed.\n",
    "\n",
    "> ### Inputs\n",
    "\n",
    "- **dataframe**: The dataset containing predictor variables.\n",
    "- **target_col**: The target column (dependent variable) to be predicted.\n",
    "- **n_trial**: Number of trials (iterations) for hyperparameter tuning.\n",
    "\n",
    "\n",
    "> ### HistGradientBoostingRegressor\n",
    "\n",
    "Executes hyperparameter optimization for the HistGradientBoostingRegressor model.\n",
    "\n",
    "> ### LGBMRegressor\n",
    "\n",
    "Performs hyperparameter optimization for the LGBMRegressor model.\n",
    "\n",
    "> ### XGBRegressor\n",
    "\n",
    "Executes hyperparameter optimization for the XGBRegressor model.\n",
    "\n",
    "> ### CatBoostRegressor\n",
    "\n",
    "Conducts hyperparameter tuning for the CatBoostRegressor model.\n",
    "\n",
    "> ### Operations\n",
    "\n",
    "- Utilizes Optuna's optimization functionalities to search for optimal hyperparameters.\n",
    "- Fits regression models with different hyperparameter configurations.\n",
    "- Evaluates models using Root Mean Squared Error (RMSE) metric.\n",
    "- Identifies and returns the best hyperparameters found during the optimization process.\n",
    "\n",
    "> ### Prints\n",
    "\n",
    "Logs messages indicating te optimization process for each regression model.\n",
    "\n",
    "> ### Returns\n",
    "\n",
    "Best hyperparameters discovered through the optimization process for the specified regression model.\n",
    "\n",
    "> ### Purpose\n",
    "\n",
    "Enhances regression models by identifying optimal hyperparameters, improving predictive performance.\n",
    "\n",
    "This class facilitates the optimization of hyperparameters for regression models, aiming to enhance their performance and accuracy in predicting target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optuna:\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def process(self, dataframe, target_col, n_trial):\n",
    "        X = dataframe.drop(columns=[target_col]) # Select independent variables on dataframe\n",
    "        y = dataframe[target_col] # Select dependent variables on dataframe\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Split the data for training and test\n",
    "        \n",
    "        if self.cfg.histgbr_optuna:\n",
    "            # Optuna for HistGradientBoostingRegressor\n",
    "            print('Optuna is executing for HistGradientBoostingRegressor...')\n",
    "            def hist_gradient_objective(trial):\n",
    "                # Hyperparameters to optimize\n",
    "                params = {\n",
    "                    'loss': 'squared_error',\n",
    "                    'scoring': 'loss',\n",
    "                    'verbose': 0,\n",
    "                    'max_iter': trial.suggest_int('max_iter', 100, 2000),\n",
    "                    'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.25, log=True),\n",
    "                    'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 2, 256),\n",
    "                    'min_samples_leaf': trial.suggest_int('min_samples_leaf', 2, 200),\n",
    "                    'max_depth': trial.suggest_int('max_depth', 2, 9),\n",
    "                    'l2_regularization': trial.suggest_float('l2_regularization', 1e-5, 10),\n",
    "                }\n",
    "                \n",
    "                # Create HistGradientBoostingRegressor with suggested parameters\n",
    "                model = HistGradientBoostingRegressor(**params)\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "                rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "                return rmse\n",
    "            \n",
    "            study_lgbm = optuna.create_study(direction='minimize')\n",
    "            study_lgbm.optimize(hist_gradient_objective, n_trials=n_trial)\n",
    "            best_params = study_lgbm.best_params\n",
    "\n",
    "            return best_params\n",
    "        \n",
    "\n",
    "        elif self.cfg.lgb_optuna:\n",
    "            # Optuna for LGBMRegressor\n",
    "            print('Optuna is executing for LGBMRegressor...')\n",
    "            def lgbm_objective(trial):\n",
    "                # Hyperparameters to optimize\n",
    "                params = {\n",
    "                    'objective': 'regression',\n",
    "                    'metric': 'rmse',\n",
    "                    'verbosity': -1,\n",
    "                    'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt', 'dart']),\n",
    "                    'n_estimators': trial.suggest_int('n_estimators', 100, 2000),\n",
    "                    'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.25, log=True),\n",
    "                    'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "                    'max_depth': trial.suggest_int('max_depth', 3, 9),\n",
    "                    'reg_alpha': trial.suggest_float('reg_alpha', 1e-5, 10),\n",
    "                    'reg_lambda': trial.suggest_float('reg_lambda', 1e-5, 10),\n",
    "                    'min_child_samples': trial.suggest_int('min_child_samples', 5, 100)\n",
    "                }\n",
    "\n",
    "                # Create LGBMRegressor with suggested parameters\n",
    "                model = lgb.LGBMRegressor(**params)\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "                rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "                return rmse\n",
    "            \n",
    "            study_lgbm = optuna.create_study(direction='minimize')\n",
    "            study_lgbm.optimize(lgbm_objective, n_trials=n_trial)\n",
    "            best_params = study_lgbm.best_params\n",
    "\n",
    "            return best_params\n",
    "        \n",
    "\n",
    "        elif self.cfg.xgb_optuna:\n",
    "            # Optuna for XGBRegressor\n",
    "            print('Optuna is executing for XGBRegressor...')\n",
    "            def xgb_objective(trial):\n",
    "                # Hyperparameters to optimize\n",
    "                params = {\n",
    "                    'objective': 'reg:squarederror',\n",
    "                    'verbosity': 0,\n",
    "                    'eval_metric': 'rmse',\n",
    "                    'boosting_type': trial.suggest_categorical('boosting_type', ['gblinear', 'dart']),\n",
    "                    'n_estimators': trial.suggest_int('n_estimators', 100, 2000),\n",
    "                    'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.25, log=True),\n",
    "                    'max_depth': trial.suggest_int('max_depth', 3, 9),\n",
    "                    'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
    "                    'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
    "                    'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "                    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0)\n",
    "                }\n",
    "\n",
    "                # Create XGBRegressor with suggested parameters\n",
    "                model = xgb.XGBRegressor(**params)\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "                rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "                return rmse\n",
    "\n",
    "            study_xgb = optuna.create_study(direction='minimize')\n",
    "            study_xgb.optimize(xgb_objective, n_trials=n_trial)\n",
    "            best_params = study_xgb.best_params\n",
    "\n",
    "            return best_params\n",
    "        \n",
    "\n",
    "        elif self.cfg.catb_optuna:\n",
    "            # Optuna for CatBoostRegressor\n",
    "            print('Optuna is executing for CatBoostRegressor...')\n",
    "            def catboost_objective(trial):\n",
    "                # Hyperparameters to optimize\n",
    "                params = {\n",
    "                    'objective': 'RMSE',\n",
    "                    'verbose': False,\n",
    "                    'eval_metric': 'RMSE',\n",
    "                    'grow_policy': 'Lossguide',\n",
    "                    'iterations': trial.suggest_int('iterations', 100, 2000),\n",
    "                    'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.25, log=True),\n",
    "                    'depth': trial.suggest_int('depth', 3, 9),\n",
    "                    'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-3, 10.0),\n",
    "                    'random_strength': trial.suggest_float('random_strength', 1e-3, 10.0),\n",
    "                    'max_leaves': trial.suggest_int('max_leaves', 10, 200),\n",
    "                    'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 20)\n",
    "                }\n",
    "                \n",
    "                # Create CatBoostRegressor with suggested parameters\n",
    "                model = catb.CatBoostRegressor(**params)\n",
    "                model.fit(X_train, y_train, eval_set=(X_test, y_test), verbose=0)\n",
    "                y_pred = model.predict(X_test)\n",
    "                rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "                return rmse\n",
    "\n",
    "            study_catboost = optuna.create_study(direction='minimize')\n",
    "            study_catboost.optimize(catboost_objective, n_trials=n_trial)\n",
    "            best_params = study_catboost.best_params\n",
    "\n",
    "            return best_params\n",
    "        \n",
    "        else:\n",
    "            # When no Optuna process is enabled\n",
    "            print('All Optuna processes are disabled. Process passed')\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Machine Learning Methods**\n",
    "\n",
    "This class is responsible for executing Machine Learning models for regression tasks.\n",
    "\n",
    "> ### Attributes\n",
    "\n",
    "- **cfg**: Configuration settings determining the models to be processed.\n",
    "\n",
    "> ### Inputs\n",
    "\n",
    "- **dataframe**: The dataset containing predictor variables and the target column.\n",
    "- **target_col**: The target column (dependent variable) to be predicted.\n",
    "- **params**: Hyperparameters for the selected models.\n",
    "\n",
    "\n",
    "> ### HistGradientBoostingRegressor\n",
    "\n",
    "Validates the HistGradientBoostingRegressor model.\n",
    "\n",
    "> ### LGBMRegressor\n",
    "\n",
    "Validates the LGBMRegressor model.\n",
    "\n",
    "> ### XGBRegressor\n",
    "\n",
    "Validates the XGBRegressor model.\n",
    "\n",
    "> ### CatBoostRegressor\n",
    "\n",
    "Executes the CatBoostRegressor model.\n",
    "\n",
    "> ### Operations\n",
    "\n",
    "- Fits regression models with specified hyperparameters on training data.\n",
    "- Evaluates model performance using Root Mean Squared Error (RMSE), R-squared (R2), and Mean Absolute Error (MAE) metrics.\n",
    "- Logs messages indicating the execution of each model and displays evaluation metrics.\n",
    "\n",
    "> ### Returns\n",
    "\n",
    "The original dataframe.\n",
    "\n",
    "> ### Purpose\n",
    "\n",
    "Executes and evaluates different regression models to predict the target variable, providing insights into model performance.\n",
    "\n",
    "This class aids in the execution and evaluation of various regression models, providing insights into their predictive performance concerning the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLModels:\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def process(self, dataframe, target_col, params):\n",
    "        # Splitting features and target\n",
    "        X = dataframe.drop(columns=target_col)\n",
    "        y = dataframe[target_col] \n",
    "\n",
    "        # Train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False, random_state=42)\n",
    "        \n",
    "        if self.cfg.histgradient:\n",
    "            # HistGradientBoostingRegressor model with best parameters from Optuna\n",
    "            model = HistGradientBoostingRegressor(**params)\n",
    "            print(f\"Validation is Executing with {type(model).__name__}\\n\")\n",
    "\n",
    "            # Training the model\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # Predictions and evaluation\n",
    "            y_pred_test = model.predict(X_test)\n",
    "            rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "            r2_test = r2_score(y_test, y_pred_test)\n",
    "            mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "            print(f\"\\n RMSE: {(rmse_test):.4f}   R2: {(r2_test):.4f}   MAE: {(mae_test):.4f}\")\n",
    "\n",
    "        elif self.cfg.lightgbm:\n",
    "            # LGBMRegressor model with best parameters from Optuna\n",
    "            model = lgb.LGBMRegressor(**params, verbosity=-1)\n",
    "            print(f\"Validation is Executing with {type(model).__name__}\\n\")\n",
    "\n",
    "            # Training the model\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # Predictions and evaluation\n",
    "            y_pred_test = model.predict(X_test)\n",
    "            rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "            r2_test = r2_score(y_test, y_pred_test)\n",
    "            mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "            print(f\"\\n RMSE: {(rmse_test):.4f}   R2: {(r2_test):.4f}   MAE: {(mae_test):.4f}\")\n",
    "\n",
    "        elif self.cfg.xgboost:\n",
    "            # XGBRegressor model with best parameters from Optuna\n",
    "            model = xgb.XGBRegressor(**params)\n",
    "            print(f\"Validation is Executing with {type(model).__name__}\")\n",
    "            \n",
    "            # Training the model\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Predictions and evaluation\n",
    "            y_pred_test = model.predict(X_test)\n",
    "            rmse_test = mean_squared_error(y_test, y_pred_test, squared=False)\n",
    "            r2_test = r2_score(y_test, y_pred_test)\n",
    "            mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "            print(f\"\\n RMSE: {(rmse_test):.4f}   R2: {(r2_test):.4f}   MAE: {(mae_test):.4f}\")\n",
    "\n",
    "        elif self.cfg.catboost:\n",
    "            # CatBoostRegressor model with best parameters from Optuna\n",
    "            params['verbose'] = False\n",
    "            params['grow_policy'] = 'Lossguide'\n",
    "            model = catb.CatBoostRegressor(**params)\n",
    "            print(f\"is Executing with {type(model).__name__}\")\n",
    "\n",
    "        else:\n",
    "            print('All ML models are disabled. Process passed...')\n",
    "        \n",
    "        return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Deep Learning Methods**\n",
    "\n",
    "These are functions for building LSTM, GRU, and combined LSTM-GRU models using TensorFlow's Keras API for sequence prediction tasks. They utilize hyperparameters for tuning the architecture and optimization of the models.\n",
    "\n",
    "### LSTM Model\n",
    "\n",
    "> ### Architecture\n",
    "\n",
    "- Three LSTM layers with different units (hyperparameter).\n",
    "- Dropout layers to mitigate overfitting.\n",
    "- Dense output layer.\n",
    "\n",
    "> ### Hyperparameters\n",
    "\n",
    "- **units**: Number of LSTM units in each layer.\n",
    "- **dropout**: Dropout rate for regularization.\n",
    "- **learning_rate**: Learning rate for the Adam optimizer.\n",
    "\n",
    "> ### Compilation\n",
    "\n",
    "- **Optimizer**: Adam\n",
    "- **Loss**: Mean Squared Error\n",
    "\n",
    "## GRU Model\n",
    "\n",
    "> ### Architecture\n",
    "\n",
    "- Three GRU layers with varying units (hyperparameter).\n",
    "- Dropout layers to prevent overfitting.\n",
    "- Dense output layer.\n",
    "\n",
    "> ### Hyperparameters\n",
    "\n",
    "- **units**: Number of GRU units in each layer.\n",
    "- **dropout**: Dropout rate for regularization.\n",
    "- **learning_rate**: Learning rate for the Adam optimizer.\n",
    "\n",
    "> ### Compilation\n",
    "\n",
    "- **Optimizer**: Adam\n",
    "- **Loss**: Mean Squared Error\n",
    "\n",
    "## LSTM-GRU Model\n",
    "\n",
    "> ### Architecture\n",
    "\n",
    "- Alternating LSTM and GRU layers with different units (hyperparameter).\n",
    "- Dropout layers after each LSTM/GRU layer.\n",
    "- Dense output layer.\n",
    "\n",
    "> ### Hyperparameters\n",
    "\n",
    "- **units**: Number of units in each LSTM and GRU layer.\n",
    "- **dropout**: Dropout rate for regularization.\n",
    "- **learning_rate**: Learning rate for the Adam optimizer.\n",
    "\n",
    "> ### Compilation\n",
    "\n",
    "- **Optimizer**: Adam\n",
    "- **Loss**: Mean Squared Error\n",
    "\n",
    "These functions allow you to build LSTM, GRU, or hybrid LSTM-GRU models for sequence prediction tasks with flexibility in architecture via hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build an LSTM model\n",
    "def build_lstm_model(hp, X):\n",
    "    \"\"\"\n",
    "    Builds an LSTM model with hyperparameters.\n",
    "    \n",
    "    Args:\n",
    "    - hp: Hyperparameters to be tuned by the optimization process.\n",
    "    - X: Input data shape.\n",
    "    \n",
    "    Returns:\n",
    "    - LSTM model architecture compiled with specified optimizer and loss.\n",
    "    \"\"\"\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.LSTM(units=hp.Int('units', min_value=64, max_value=256, step=64), \n",
    "                                   return_sequences=True, \n",
    "                                   input_shape=(X.shape[1], X.shape[2]),\n",
    "                                   kernel_regularizer=tf.keras.regularizers.l2(hp.Choice('l2_rate', values=[0.001, 0.01, 0.1]))))\n",
    "    # Add a dropout layer\n",
    "    model.add(tf.keras.layers.Dropout(hp.Float('dropout', min_value=0.1, max_value=0.3, step=0.1)))\n",
    "    # Add another LSTM layer\n",
    "    model.add(tf.keras.layers.LSTM(units=int(hp.Int('units', min_value=64, max_value=256, step=64)) // 2, \n",
    "                                   return_sequences=True, \n",
    "                                   activation='relu',\n",
    "                                   kernel_regularizer=tf.keras.regularizers.l2(hp.Choice('l2_rate', values=[0.001, 0.01, 0.1]))))\n",
    "    model.add(tf.keras.layers.Dropout(hp.Float('dropout', min_value=0.1, max_value=0.3, step=0.1)))\n",
    "    # Final LSTM layer with units reduced to 1\n",
    "    model.add(tf.keras.layers.LSTM(units=int(hp.Int('units', min_value=64, max_value=256, step=64)) // 4, \n",
    "                                   activation='relu',\n",
    "                                   kernel_regularizer=tf.keras.regularizers.l2(hp.Choice('l2_rate', values=[0.001, 0.01, 0.1]))))\n",
    "    # Output layer\n",
    "    model.add(tf.keras.layers.Dense(units=1)) \n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-4, 1e-3, 1e-2]))\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "\n",
    "# Function to build a GRU model\n",
    "def build_gru_model(hp, X):\n",
    "    \"\"\"\n",
    "    Builds a GRU model with hyperparameters.\n",
    "    \n",
    "    Args:\n",
    "    - hp: Hyperparameters to be tuned by the optimization process.\n",
    "    - X: Input data shape.\n",
    "    \n",
    "    Returns:\n",
    "    - GRU model architecture compiled with specified optimizer and loss.\n",
    "    \"\"\"\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.GRU(units=hp.Int('units', min_value=64, max_value=256, step=64), \n",
    "                                  return_sequences=True, \n",
    "                                  input_shape=(X.shape[1], X.shape[2]),\n",
    "                                  kernel_regularizer=tf.keras.regularizers.l2(hp.Choice('l2_rate', values=[0.001, 0.01, 0.1]))))\n",
    "    # Add a dropout layer\n",
    "    model.add(tf.keras.layers.Dropout(hp.Float('dropout', min_value=0.1, max_value=0.3, step=0.1)))\n",
    "    # Add another GRU layer\n",
    "    model.add(tf.keras.layers.GRU(units=int(hp.Int('units', min_value=64, max_value=256, step=64)) // 2, \n",
    "                                  return_sequences=True, \n",
    "                                  activation='relu',\n",
    "                                  kernel_regularizer=tf.keras.regularizers.l2(hp.Choice('l2_rate', values=[0.001, 0.01, 0.1]))))\n",
    "    # Add a dropout layer\n",
    "    model.add(tf.keras.layers.Dropout(hp.Float('dropout', min_value=0.1, max_value=0.3, step=0.1)))\n",
    "    # Add another GRU layer\n",
    "    model.add(tf.keras.layers.GRU(units=int(hp.Int('units', min_value=64, max_value=256, step=64)) // 4, \n",
    "                                  activation='relu',\n",
    "                                  kernel_regularizer=tf.keras.regularizers.l2(hp.Choice('l2_rate', values=[0.001, 0.01, 0.1]))))\n",
    "    # Output layer\n",
    "    model.add(tf.keras.layers.Dense(units=1)) \n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-4, 1e-3, 1e-2]))\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "\n",
    "# Function to build a combined LSTM-GRU model\n",
    "def build_lstm_gru_model(hp, X):\n",
    "    \"\"\"\n",
    "    Builds a combined LSTM-GRU model with hyperparameters.\n",
    "    \n",
    "    Args:\n",
    "    - hp: Hyperparameters to be tuned by the optimization process.\n",
    "    - X: Input data shape.\n",
    "    \n",
    "    Returns:\n",
    "    - Combined LSTM-GRU model architecture compiled with specified optimizer and loss.\n",
    "    \"\"\"\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.LSTM(units=hp.Int('units', min_value=64, max_value=256, step=64),\n",
    "                                   return_sequences=True,\n",
    "                                   input_shape=(X.shape[1], X.shape[2]),\n",
    "                                   kernel_regularizer=tf.keras.regularizers.l2(hp.Choice('l2_rate', values=[0.001, 0.01, 0.1]))))\n",
    "    model.add(tf.keras.layers.Dropout(hp.Float('dropout', min_value=0.1, max_value=0.3, step=0.1)))\n",
    "    model.add(tf.keras.layers.GRU(units=hp.Int('units', min_value=64, max_value=256, step=64) // 2,\n",
    "                                   return_sequences=True,\n",
    "                                   activation='relu',\n",
    "                                   kernel_regularizer=tf.keras.regularizers.l2(hp.Choice('l2_rate', values=[0.001, 0.01, 0.1]))))\n",
    "    model.add(tf.keras.layers.Dropout(hp.Float('dropout', min_value=0.1, max_value=0.3, step=0.1)))\n",
    "    model.add(tf.keras.layers.LSTM(units=hp.Int('units', min_value=64, max_value=256, step=64) // 4,\n",
    "                                   return_sequences=True,\n",
    "                                   activation='relu',\n",
    "                                   kernel_regularizer=tf.keras.regularizers.l2(hp.Choice('l2_rate', values=[0.001, 0.01, 0.1]))))\n",
    "    model.add(tf.keras.layers.Dropout(hp.Float('dropout', min_value=0.1, max_value=0.3, step=0.1)))\n",
    "    model.add(tf.keras.layers.GRU(units=hp.Int('units', min_value=64, max_value=256, step=64) // 8,\n",
    "                                   activation='relu',\n",
    "                                   kernel_regularizer=tf.keras.regularizers.l2(hp.Choice('l2_rate', values=[0.001, 0.01, 0.1]))))\n",
    "    model.add(tf.keras.layers.Dense(units=1))\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-4, 1e-3, 1e-2]))\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **KerasTuner Method**\n",
    "\n",
    "This KerasTuner class is aimed at hyperparameter tuning for different types of neural network models used for sequence prediction tasks.\n",
    "\n",
    "\n",
    "> ### Initialization\n",
    "\n",
    "Takes a configuration parameter (cfg) to define which tuner process is enabled.\n",
    "\n",
    "> ### Methods\n",
    "\n",
    "- **process**: Conducts hyperparameter tuning for LSTM, GRU, or LSTM-GRU hybrid models based on the enabled tuner configuration.\n",
    "- **dataframe**: The input data.\n",
    "- **target_col**: The target column for prediction.\n",
    "\n",
    "> ### Functionality\n",
    "\n",
    "- Uses Keras Tuner's RandomSearch to search through the hyperparameter space for each model type.\n",
    "- Searches for the best hyperparameters for LSTM, GRU, or LSTM-GRU models based on the enabled tuner configuration (lstm_tuner, gru_tuner, lstm_gru_tuner).\n",
    "- Performs the hyperparameter search by evaluating the models on validation data (using early stopping to prevent overfitting).\n",
    "\n",
    "> ### Returned Values\n",
    "\n",
    "Returns the best hyperparameters found for the selected model type.\n",
    "\n",
    "> ### Output\n",
    "\n",
    "Prints the best hyperparameters discovered for each type of model after the search process.\n",
    "\n",
    "This class provides an automated way to search for the best hyperparameters for LSTM, GRU, or LSTM-GRU models using Keras Tuner, facilitating the optimization of these models for sequence prediction tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasTuner:\n",
    "    def __init__(self, cfg):\n",
    "        \"\"\"\n",
    "        Initialize KerasTuner class with configuration parameters.\n",
    "\n",
    "        Args:\n",
    "        - cfg: Configuration parameters for KerasTuner.\n",
    "        \"\"\"\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def process(self, dataframe, target_col):\n",
    "        \"\"\"\n",
    "        Perform hyperparameter tuning using KerasTuner.\n",
    "\n",
    "        Args:\n",
    "        - dataframe: Input dataframe for training.\n",
    "        - target_col: Column containing the target variable.\n",
    "\n",
    "        Returns:\n",
    "        - best_hps: Best hyperparameters found by KerasTuner.\n",
    "        \"\"\"\n",
    "\n",
    "        # Split independent variables and target\n",
    "        X = dataframe.drop(columns=target_col).values\n",
    "        y = dataframe[target_col].values\n",
    "        X = X.reshape((X.shape[0], 1, X.shape[1]))\n",
    "\n",
    "        # Train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False, random_state=42)\n",
    "        \n",
    "        if self.cfg.lstm_tuner:\n",
    "            # KerasTuner for LSTM model\n",
    "            tuner = RandomSearch(lambda hp: build_lstm_model(hp, X_train), \n",
    "                                 objective='val_loss', \n",
    "                                 max_trials=50,\n",
    "                                 executions_per_trial=1)\n",
    "\n",
    "            # Tuning on dataset for the best hyperparameters\n",
    "            tuner.search(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test), \n",
    "                         callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1)])\n",
    "            best_hps = tuner.get_best_hyperparameters(num_trials=1)[0] # Pick best parameters \n",
    "            print(\"Best LSTM parameters:\", best_hps.values)\n",
    "\n",
    "            return best_hps\n",
    "\n",
    "        elif self.cfg.gru_tuner:\n",
    "            # KerasTuner for GRU model\n",
    "            tuner = RandomSearch(lambda hp: build_gru_model(hp, X_train), \n",
    "                                 objective='val_loss', \n",
    "                                 max_trials=50,\n",
    "                                 executions_per_trial=1)\n",
    "            \n",
    "            # Tuning on dataset for the best hyperparameters\n",
    "            tuner.search(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test), \n",
    "                         callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1)])\n",
    "            best_hps = tuner.get_best_hyperparameters(num_trials=1)[0] # Pick best parameters\n",
    "            print(\"Best GRU parameters:\", best_hps.values)\n",
    "\n",
    "            return best_hps\n",
    "\n",
    "        elif self.cfg.lstm_gru_tuner:\n",
    "            # KerasTuner for combined LSTM-GRU model\n",
    "            tuner = RandomSearch(lambda hp: build_lstm_gru_model(hp, X_train), \n",
    "                                 objective='val_loss', \n",
    "                                 max_trials=50,\n",
    "                                 executions_per_trial=1)\n",
    "            \n",
    "            # Tuning on dataset for the best hyperparameters\n",
    "            tuner.search(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test), \n",
    "                         callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1)])\n",
    "            best_hps = tuner.get_best_hyperparameters(num_trials=1)[0] # Pick best parameters\n",
    "            print(\"Best LSTM-GRU parameters:\", best_hps.values)\n",
    "\n",
    "            return best_hps\n",
    "        \n",
    "        else:\n",
    "            print(\"KerasTuner process is disabled. Process passed..\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Test Prediction Deep Learning Method**\n",
    "\n",
    "This DLModels class is designed to build and train different deep learning models for sequence prediction tasks using LSTM, GRU, or LSTM-GRU architectures based on the configuration settings.\n",
    "\n",
    "> ### Initialization\n",
    "\n",
    "Takes a configuration parameter (cfg) to determine which DL model is enabled for processing.\n",
    "\n",
    "> ### Methods\n",
    "\n",
    "- **process**: Builds and trains LSTM, GRU, or LSTM-GRU models based on the enabled model configuration (lstm_model, gru_model, lstm_gru_model).\n",
    "- **dataframe**: The training data.\n",
    "- **dataframe_test**: The test data.\n",
    "- **target_col**: The target column for prediction.\n",
    "- **unit**: The number of units/neurons in the layers.\n",
    "- **dropout_rate**: The dropout rate for regularization.\n",
    "- **l2_rate**: The L2 regularization rate.\n",
    "- **learning_rate**: The learning rate for optimization.\n",
    "\n",
    "> ### Functionality\n",
    "\n",
    "- Builds and trains LSTM, GRU, or LSTM-GRU models based on the enabled model configuration using TensorFlow/Keras.\n",
    "- Each model architecture consists of multiple recurrent layers with dropout and optional regularization.\n",
    "- Uses Adam or Nadam optimizer and mean squared error loss.\n",
    "- Trains the models for 20 epochs with a batch size of 32 and early stopping based on loss metric.\n",
    "\n",
    "> ### Returned Values\n",
    "\n",
    "Returns the predicted values for the test data from the trained models.\n",
    "\n",
    "> ### Output\n",
    "\n",
    "- Prints the execution status of the selected DL model (LSTM, GRU, or LSTM-GRU).\n",
    "\n",
    "This class facilitates the creation and training of deep learning models for sequence prediction tasks, allowing you to select different architectures and hyperparameters based on your dataset and problem requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DLModels:\n",
    "    def __init__(self, cfg):\n",
    "        \"\"\"\n",
    "        Initializes the DLModels class with a configuration object.\n",
    "\n",
    "        Args:\n",
    "        - cfg: Configuration object containing model flags (e.g., lstm_model, gru_model, lstm_gru_model).\n",
    "        \"\"\"\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def process(self, dataframe, dataframe_test, target_col, unit, dropout_rate, l2_rate, learning_rate):\n",
    "        \"\"\"\n",
    "        Processes the data and executes the selected deep learning model based on configuration flags.\n",
    "\n",
    "        Args:\n",
    "        - dataframe: Training data in pandas DataFrame format.\n",
    "        - dataframe_test: Test data in pandas DataFrame format.\n",
    "        - target_col: Name of the target column.\n",
    "        - unit: Number of units/neurons in the layers.\n",
    "        - dropout_rate: Dropout rate for regularization.\n",
    "        - l2_rate: L2 regularization rate.\n",
    "        - learning_rate: Learning rate for optimization.\n",
    "\n",
    "        Returns:\n",
    "        - Predictions if a model is executed; otherwise, returns the original dataframe.\n",
    "        \"\"\"\n",
    "\n",
    "        # Extracting features and target variable from the training data\n",
    "        X = dataframe.drop(columns=target_col).values\n",
    "        y = dataframe[target_col].values\n",
    "\n",
    "        # Reshaping input data to be compatible with LSTM/GRU input shape\n",
    "        X = X.reshape((X.shape[0], 1, X.shape[1]))\n",
    "\n",
    "        # Extracting features from the test data and reshaping it\n",
    "        test = dataframe_test.drop(columns=target_col).values\n",
    "        test = test.reshape((test.shape[0], 1, test.shape[1]))\n",
    "\n",
    "        # Model selection based on configuration flags\n",
    "        if self.cfg.lstm_model:\n",
    "            # LSTM Model\n",
    "            print(\"LSTM Model is executing..\\n\")\n",
    "            \n",
    "            # Initializing an LSTM model\n",
    "            model = tf.keras.models.Sequential()\n",
    "            model.add(tf.keras.layers.LSTM(units=unit, return_sequences=True, input_shape=(X.shape[1], X.shape[2]),\n",
    "                                           kernel_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
    "            model.add(tf.keras.layers.Dropout(dropout_rate))  \n",
    "            \n",
    "            model.add(tf.keras.layers.LSTM(units=unit//2, return_sequences=True, activation='relu',\n",
    "                                           kernel_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
    "            model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "            model.add(tf.keras.layers.LSTM(units=unit//4, activation='relu', \n",
    "                                           kernel_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
    "            \n",
    "            model.add(tf.keras.layers.Dense(units=1)) \n",
    "\n",
    "            # Compiling the model\n",
    "            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mean_squared_error')\n",
    "\n",
    "            # Training the model\n",
    "            model.fit(X, y, epochs=20, batch_size=32, \n",
    "                      callbacks=[tf.keras.callbacks.EarlyStopping(monitor='loss',  patience=2)])\n",
    "\n",
    "            # Making predictions\n",
    "            y_pred = model.predict(test)\n",
    "\n",
    "            return y_pred\n",
    "\n",
    "        elif self.cfg.gru_model:\n",
    "            # GRU Model\n",
    "            print(\"GRU Model is executing..\\n\")\n",
    "\n",
    "            # Initializing a GRU model\n",
    "            model = tf.keras.models.Sequential()\n",
    "            model.add(tf.keras.layers.GRU(units=unit, return_sequences=True, input_shape=(X.shape[1], X.shape[2]), \n",
    "                                          kernel_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
    "            model.add(tf.keras.layers.Dropout(dropout_rate)) \n",
    "\n",
    "            model.add(tf.keras.layers.GRU(units=unit//2, return_sequences=True, activation='relu', \n",
    "                                          kernel_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
    "            model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "            model.add(tf.keras.layers.GRU(units=unit//4, return_sequences=True, activation='relu', \n",
    "                                          kernel_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
    "            \n",
    "            model.add(tf.keras.layers.Dense(units=1)) \n",
    "\n",
    "            # Compiling the model\n",
    "            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mean_squared_error')\n",
    "\n",
    "            # Training the model\n",
    "            model.fit(X, y, epochs=20, batch_size=32, \n",
    "                      callbacks=[tf.keras.callbacks.EarlyStopping(monitor='loss',  patience=2)])\n",
    "        \n",
    "            # Making predictions\n",
    "            y_pred = model.predict(test)\n",
    "\n",
    "            return y_pred\n",
    "\n",
    "        elif self.cfg.lstm_gru_model:\n",
    "            # LSTM-GRU Model\n",
    "            print(\"LSTM-GRU Model is executing..\\n\")\n",
    "\n",
    "            # Initializing an LSTM-GRU model\n",
    "            model = tf.keras.models.Sequential()\n",
    "            model.add(tf.keras.layers.LSTM(units=unit, return_sequences=True, input_shape=(X.shape[1], X.shape[2]), \n",
    "                                           kernel_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
    "            model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "            model.add(tf.keras.layers.GRU(units=unit//2, return_sequences=True, activation='relu', \n",
    "                                          kernel_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
    "            model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "            model.add(tf.keras.layers.LSTM(units=unit//4, return_sequences=True, activation='relu',\n",
    "                                           kernel_regularizer=tf.keras.regularizers.l2(l2_rate)))\n",
    "            model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "            model.add(tf.keras.layers.GRU(units=unit//8, activation='relu', \n",
    "                                            kernel_regularizer=tf.keras.regularizers.l2(l2_rate))) \n",
    "            \n",
    "            model.add(tf.keras.layers.Dense(units=1)) \n",
    "\n",
    "            # Compiling the model\n",
    "            model.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=learning_rate), loss='mean_squared_error')\n",
    "\n",
    "            # Training the model\n",
    "            model.fit(X, y, epochs=20, batch_size=32, \n",
    "                      callbacks=[tf.keras.callbacks.EarlyStopping(monitor='loss',  patience=2)])\n",
    "\n",
    "            # Making predictions\n",
    "            y_pred = model.predict(test)\n",
    "\n",
    "            return y_pred\n",
    "\n",
    "        else:\n",
    "            # No model selected\n",
    "            print(\"All Models are disabled. Returning original dataframe..\")\n",
    "            return dataframe\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
